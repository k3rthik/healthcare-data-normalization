# üç¨ Project 2: Batch Processing ETL Pipeline for Tiger‚Äôs Candy Store üç¨

## Overview

Tiger‚Äôs Candy, a popular candy store originating from the RIT campus, has seen substantial growth. To manage its expanding operations, the store needs an automated system that processes online orders in batches.

This project implements a batch processing ETL (Extract, Transform, Load) pipeline to handle raw order transactions on a daily basis. The pipeline performs essential tasks such as:
	‚Ä¢	Validating transaction details ‚úÖ
	‚Ä¢	Checking inventory levels üì¶
	‚Ä¢	Forecasting sales and profits üìä

### üìä Dataset Overview

The dataset includes various information related to customers, products, and order transactions.

üë• Customers Dataset
	‚Ä¢	customer_id: Unique identifier for each customer
	‚Ä¢	first_name: First name of the customer
	‚Ä¢	last_name: Last name of the customer
	‚Ä¢	email: Customer‚Äôs email address
	‚Ä¢	address: Physical address of the customer
	‚Ä¢	phone: Phone number of the customer

üç´ Products Dataset
	‚Ä¢	product_id: Unique identifier for each product
	‚Ä¢	product_name: Name of the product
	‚Ä¢	product_category: Category of candy (e.g., Chocolate, Gummy)
	‚Ä¢	sales_price: Retail price of the product
	‚Ä¢	cost_to_make: Manufacturing cost of the product
	‚Ä¢	stock: Inventory level of the product

 Order Transactions (Raw JSON Format)

 Each transaction file consists of:

    
    {
    "transaction_id": 73434473,
    "customer_id": 29,
    "timestamp": "2024-02-02T12:00:40.808092",
    "items": [
        {"product_id": 17, "product_name": "Candy A", "qty": 5},
        {"product_id": 3, "product_name": "Candy B", "qty": 2}
    ]
    }
    

### Required Software Packages

To set up your environment for the project, the following packages must be installed:

    
    pip install apache-airflow
    pip install pyspark
    pip install python-dotenv
    pip install prophet
    

### üìù Setup Instructions and Expected Outputs

1. Setting up the Virtual Environment

‚Ä¢	Create a virtual environment in your project directory:

    
    python3 -m venv venv
    
‚Ä¢	Activate the virtual environment:

    
    source venv/bin/activate
    

2. Running the Code

‚Ä¢	After activating the virtual environment, run the main script from the project-2 directory:

    
    python3 src/main.py
    

### Airflow DAG Setup

1. Install Apache Airflow in your virtual environment:

    ```bash
    pip install apache-airflow
    ```

2. Initialize the Airflow database:

    ```bash
    airflow db init
    ```
3. Create an admin user account and set the password:

    ```bash
    airflow users create --username admin --firstname yourName --lastname yourLastName --role Admin --email yourEmail@example.com
    ```
4. Start the Airflow webserver and scheduler in two different terminals:

    
	‚Ä¢	Terminal 1 (Webserver):
    
    ```bash
    airflow webserver --port 8080
    ```

    ‚Ä¢	Terminal 2 (Scheduler):

    ```bash
    airflow scheduler
    ```
5. Copy the DAG script (order_processing_dag.py) to the ~/airflow/dags directory.

6. Access Airflow UI:
	‚Ä¢	Open a web browser and visit http://localhost:8080.
	‚Ä¢	Log in with your credentials.
	‚Ä¢	Locate order_processing_dag in the DAGs list and toggle it to make it active.
	‚Ä¢	Open the order_processing_dag and click the Play button at the top right to run it.

üìÇ Project Structure
	‚Ä¢	src/
	‚Ä¢	data_processor.py: Contains methods for processing data (e.g., validating transactions, checking inventory, etc.)
	‚Ä¢	main.py: Main script that runs the ETL pipeline and orchestrates tasks.
	‚Ä¢	time_series.py: Contains logic for forecasting sales and profits.
	‚Ä¢	data/
	‚Ä¢	candy_store.sql: MySQL database dump for customers, products, and transactions.
	‚Ä¢	transactions.json: MongoDB raw order transaction data.
	‚Ä¢	.env.example: Template for environment variables (fill in your own database credentials).
	‚Ä¢	config.yaml: Configuration file for specifying various pipeline parameters like paths and database connections.

